import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc

st.set_page_config(
    page_title="ä¹³è…ºç™Œé£Žé™©é¢„æµ‹ç³»ç»Ÿ",
    page_icon="ðŸ©º",
    layout="wide"
)

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

@st.cache_data
def load_data():
    data = load_breast_cancer()
    df = pd.DataFrame(data.data, columns=data.feature_names)
    df['target'] = data.target
    return df, data.target_names

def sidebar_layout(df):
    st.sidebar.title("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    st.sidebar.subheader("ðŸ“Š æ¨¡åž‹é€‰æ‹©")
    model_list = ["Logistic Regression", "Support Vector Machine (SVM)", "K-Nearest Neighbors (KNN)", "Decision Tree"]
    selected_model = st.sidebar.selectbox("é€‰æ‹©ç®—æ³•", model_list)

    model_params = {}
    if selected_model == "K-Nearest Neighbors (KNN)":
        k_value = st.sidebar.slider("K å€¼", 1, 20, 5)
        model_params['k'] = k_value
    elif selected_model == "Decision Tree":
        max_depth = st.sidebar.slider("æœ€å¤§æ·±åº¦", 1, 20, 5)
        model_params['max_depth'] = max_depth
    elif selected_model == "Support Vector Machine (SVM)":
        C_value = st.sidebar.slider("æ­£åˆ™åŒ–ç³»æ•° (C)", 0.01, 10.0, 1.0)
        model_params['C'] = C_value
    
    st.sidebar.subheader("ðŸ“ˆ æµ‹è¯•é›†æ¯”ä¾‹")
    split_size = st.sidebar.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)

    st.sidebar.subheader("ðŸ©º æ‚£è€…ç‰¹å¾è¾“å…¥")
    st.sidebar.info("è°ƒæ•´ä¸‹æ–¹æ»‘å—è¾“å…¥æ‚£è€…æŒ‡æ ‡")
    
    user_input = {}
    feature_columns = df.columns[:-1]
    top_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness']
    
    for col in top_features:
        min_val = float(df[col].min())
        max_val = float(df[col].max())
        mean_val = float(df[col].mean())
        user_input[col] = st.sidebar.slider(f"{col}", min_val, max_val, mean_val)
    
    return split_size, selected_model, model_params, user_input, feature_columns

def plot_feature_importance(model, feature_names, model_name, X_val, y_val):
    importances = None
    
    if model_name == "Decision Tree":
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
    elif model_name == "Logistic Regression" or model_name == "Support Vector Machine (SVM)":
        if hasattr(model, 'coef_'):
            importances = np.abs(model.coef_[0])
            
    if importances is None:
        from sklearn.inspection import permutation_importance
        result = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42)
        importances = result.importances_mean

    feature_imp = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feature_imp = feature_imp.sort_values(by='Importance', ascending=False).head(10)

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_imp, palette='viridis', ax=ax)
    plt.title(f'ç‰¹å¾é‡è¦æ€§åˆ†æž ({model_name})')
    plt.xlabel('é‡è¦æ€§å¾—åˆ†')
    plt.ylabel('åŒ»å­¦ç‰¹å¾')
    
    return fig

def main():
    st.title("ðŸ©º ä¹³è…ºç™Œé£Žé™©é¢„æµ‹ç³»ç»Ÿ")
    
    with st.expander("ðŸ“– ç³»ç»Ÿç®€ä»‹", expanded=True):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.info("""
            **ðŸ’¡ ç³»ç»Ÿè¯´æ˜Ž**
            
            æœ¬ç³»ç»ŸåŸºäºŽæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä½¿ç”¨å¨æ–¯åº·æ˜Ÿä¹³è…ºç™Œæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯å¸®åŠ©åŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜å¿«é€Ÿè¯„ä¼°ä¹³è…ºç™Œé£Žé™©ã€‚
            """)
        with col2:
            st.success("""
            **ðŸŽ¯ ä½¿ç”¨æ–¹æ³•**
            
            1. åœ¨å·¦ä¾§é€‰æ‹©æ¨¡åž‹å’Œå‚æ•°
            2. è¾“å…¥æ‚£è€…ç‰¹å¾æ•°æ®
            3. ç‚¹å‡»"å¼€å§‹é¢„æµ‹"æŒ‰é’®
            4. æŸ¥çœ‹é¢„æµ‹ç»“æžœå’Œåˆ†æžæŠ¥å‘Š
            """)
        with col3:
            st.warning("""
            **ðŸ“Š æ•°æ®é›†ä¿¡æ¯**
            
            - æ ·æœ¬æ•°ï¼š569ä¾‹
            - ç‰¹å¾æ•°ï¼š30ä¸ªåŒ»å­¦æŒ‡æ ‡
            - ç±»åˆ«ï¼šæ¶æ€§(0) / è‰¯æ€§(1)
            - æ¥æºï¼šSklearnæ•°æ®é›†
            """)

    df, target_names = load_data()
    test_size, model_name, params, user_input_dict, all_features = sidebar_layout(df)

    X = df.drop('target', axis=1)
    y = df['target']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    clf = None
    if model_name == "Logistic Regression":
        clf = LogisticRegression(random_state=42)
    elif model_name == "Support Vector Machine (SVM)":
        clf = SVC(C=params['C'], probability=True, random_state=42)
    elif model_name == "K-Nearest Neighbors (KNN)":
        clf = KNeighborsClassifier(n_neighbors=params['k'])
    elif model_name == "Decision Tree":
        clf = DecisionTreeClassifier(max_depth=params['max_depth'], random_state=42)

    clf.fit(X_train_scaled, y_train)
    
    y_pred = clf.predict(X_test_scaled)
    y_prob = clf.predict_proba(X_test_scaled)[:, 1]
    
    accuracy = accuracy_score(y_test, y_pred)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    st.divider()
    st.header("ðŸ“Š æ¨¡åž‹æ€§èƒ½æŒ‡æ ‡")
    
    metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
    with metric_col1:
        st.metric("å½“å‰æ¨¡åž‹", model_name)
    with metric_col2:
        st.metric("å‡†ç¡®çŽ‡", f"{accuracy:.2%}")
    with metric_col3:
        st.metric("AUC å€¼", f"{roc_auc:.2f}")
    with metric_col4:
        st.metric("æµ‹è¯•é›†æ¯”ä¾‹", f"{test_size:.0%}")

    st.divider()
    st.header("ðŸ”® é¢„æµ‹ç»“æžœ")
    
    input_data = []
    feature_means = df.drop('target', axis=1).mean()
    
    for feature in all_features:
        if feature in user_input_dict:
            input_data.append(user_input_dict[feature])
        else:
            input_data.append(feature_means[feature])
            
    input_vector = np.array(input_data).reshape(1, -1)
    input_vector_scaled = scaler.transform(input_vector)

    if st.button("ï¿½ å¼€å§‹é¢„æµ‹", use_container_width=True):
        prediction = clf.predict(input_vector_scaled)[0]
        prediction_proba = clf.predict_proba(input_vector_scaled)[0]
        
        col_pred1, col_pred2 = st.columns(2)
        
        with col_pred1:
            if prediction == 0:
                st.error(f"âš ï¸ é«˜é£Žé™© (æ¶æ€§)")
                st.metric("æ¶æ€§æ¦‚çŽ‡", f"{prediction_proba[0]:.2%}")
                st.progress(int(prediction_proba[0] * 100))
            else:
                st.success(f"âœ… ä½Žé£Žé™© (è‰¯æ€§)")
                st.metric("è‰¯æ€§æ¦‚çŽ‡", f"{prediction_proba[1]:.2%}")
                st.progress(int(prediction_proba[1] * 100))
        
        with col_pred2:
            st.info("""
            **é¢„æµ‹è¯¦æƒ…**
            
            - æ¨¡åž‹ï¼š{0}
            - æµ‹è¯•é›†å‡†ç¡®çŽ‡ï¼š{1:.2%}
            - AUC å€¼ï¼š{2:.2f}
            """.format(model_name, accuracy, roc_auc))
            
            st.markdown("""
            > **é‡è¦æç¤º**ï¼šæœ¬é¢„æµ‹ç»“æžœä»…åŸºäºŽæœºå™¨å­¦ä¹ æ¨¡åž‹å®žéªŒï¼Œä¸èƒ½ä½œä¸ºçœŸå®žä¸´åºŠè¯Šæ–­ä¾æ®ã€‚è¯·éµåŒ»å˜±ã€‚
            """)

    st.divider()
    st.header("ðŸ“ˆ è¯¦ç»†åˆ†æžæŠ¥å‘Š")

    tab1, tab2, tab3 = st.tabs(["æ··æ·†çŸ©é˜µ", "ROC æ›²çº¿", "ç‰¹å¾é‡è¦æ€§"])

    with tab1:
        st.subheader("æ··æ·†çŸ©é˜µåˆ†æž")
        cm = confusion_matrix(y_test, y_pred)
        fig_cm, ax_cm = plt.subplots()
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm, 
                    xticklabels=['æ¶æ€§ (0)', 'è‰¯æ€§ (1)'], 
                    yticklabels=['æ¶æ€§ (0)', 'è‰¯æ€§ (1)'])
        plt.ylabel('çœŸå®žæ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        st.pyplot(fig_cm)
        
        st.info("""
        **æ··æ·†çŸ©é˜µè§£è¯»**ï¼š
        - å·¦ä¸Šè§’ï¼šæ­£ç¡®é¢„æµ‹ä¸ºæ¶æ€§çš„æ•°é‡
        - å³ä¸Šè§’ï¼šé”™è¯¯é¢„æµ‹ä¸ºè‰¯æ€§çš„æ•°é‡ï¼ˆæ¼è¯Šï¼‰
        - å·¦ä¸‹è§’ï¼šé”™è¯¯é¢„æµ‹ä¸ºæ¶æ€§çš„æ•°é‡ï¼ˆè¯¯è¯Šï¼‰
        - å³ä¸‹è§’ï¼šæ­£ç¡®é¢„æµ‹ä¸ºè‰¯æ€§çš„æ•°é‡
        """)

    with tab2:
        st.subheader("ROC æ›²çº¿åˆ†æž")
        fig_roc, ax_roc = plt.subplots()
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å‡é˜³æ€§çŽ‡ (False Positive Rate)')
        plt.ylabel('çœŸé˜³æ€§çŽ‡ (True Positive Rate)')
        plt.title('ROC æ›²çº¿')
        plt.legend(loc="lower right")
        st.pyplot(fig_roc)
        
        st.info("""
        **ROC æ›²çº¿è§£è¯»**ï¼š
        - æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼Œæ¨¡åž‹æ€§èƒ½è¶Šå¥½
        - AUC å€¼èŒƒå›´ï¼š0.5ï¼ˆéšæœºçŒœæµ‹ï¼‰~ 1.0ï¼ˆå®Œç¾Žåˆ†ç±»ï¼‰
        - AUC > 0.9ï¼šä¼˜ç§€ï¼›0.8-0.9ï¼šè‰¯å¥½ï¼›0.7-0.8ï¼šä¸€èˆ¬ï¼›< 0.7ï¼šè¾ƒå·®
        """)

    with tab3:
        st.subheader("ç‰¹å¾é‡è¦æ€§åˆ†æž")
        st.markdown("è¯¥å›¾å±•ç¤ºäº†æ¨¡åž‹åœ¨åˆ¤æ–­'è‰¯æ€§/æ¶æ€§'æ—¶ï¼Œè®¤ä¸ºå“ªäº›åŒ»å­¦ç‰¹å¾æœ€ä¸ºå…³é”®ã€‚")
        fig_imp = plot_feature_importance(clf, df.columns[:-1], model_name, X_test_scaled, y_test)
        st.pyplot(fig_imp)
        
        st.info("""
        **ç‰¹å¾é‡è¦æ€§è§£è¯»**ï¼š
        - **æ¡å½¢è¶Šé•¿**ï¼šä»£è¡¨è¯¥ç‰¹å¾å¯¹é¢„æµ‹ç»“æžœçš„å½±å“è¶Šå¤§
        - **åŒ»å­¦æ„ä¹‰**ï¼šä¾‹å¦‚ï¼Œå¦‚æžœ `mean concave points`ï¼ˆå¹³å‡å‡¹ç‚¹æ•°ï¼‰æŽ’åœ¨ç¬¬ä¸€ä½ï¼Œè¯´æ˜Žæ¨¡åž‹è®¤ä¸ºè¿™ä¸ªæŒ‡æ ‡æ˜¯åˆ¤æ–­ç™Œç—‡æœ€æ ¸å¿ƒçš„ä¾æ®
        - è¿™æœ‰åŠ©äºŽåŒ»ç”Ÿç†è§£æ¨¡åž‹çš„å†³ç­–è¿‡ç¨‹ï¼Œæé«˜å¯è§£é‡Šæ€§
        """)

if __name__ == "__main__":
    main()
